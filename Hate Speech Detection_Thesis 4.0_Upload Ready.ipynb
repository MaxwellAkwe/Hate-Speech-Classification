{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56467b0",
   "metadata": {},
   "source": [
    "# Libraries Needed\n",
    "\n",
    "The below codes import the necessary libraries needed to work on the project. The libraries being imported are necessary for Natural Language Processing task, data preprocessing, cleaning, data splitting, feature extraction and machine learning models implemented for this study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3728a8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tag import pos_tag #library used for POS tagging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import jupyter_dash\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8acc6d",
   "metadata": {},
   "source": [
    "## Parsing CSV files:  \n",
    "\n",
    "The below codes read data from the CSV file (dataset.csv) and displays the first five rows with headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9052a57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                        tweet  \n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2                                                                                                         bihday your majesty  \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦    \n",
       "4                                                                                      factsguide: society now    #motivation  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parse CSV file\n",
    "tweets = pd.read_csv('dataset.csv')\n",
    "columns = ['label', 'tweet']\n",
    "tweets[columns].to_csv('tweets.csv', index=False)\n",
    "tweets = pd.read_csv('tweets.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b19d4f",
   "metadata": {},
   "source": [
    "# Phase 1 - Data Exploration\n",
    "\n",
    "In this phase, we learn the attributes of distribution of our dataset to get a guide on how to drive our analysis and preprocessing\n",
    "\n",
    "The code below displays information about our tweet dataset providinga concise summary including the number of rows, the number of columns, the data type of each column, and the number of non-null values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99b3af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31962 entries, 0 to 31961\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   31962 non-null  int64 \n",
      " 1   tweet   31962 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 499.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023fae03",
   "metadata": {},
   "source": [
    "The code below checks for missing value in the data frame and prints a message indicating the number of missing value using string formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51adf18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 missing values.\n"
     ]
    }
   ],
   "source": [
    "#Check for missing values\n",
    "null_value = tweets.isnull().sum().sum()\n",
    "print(f'There are {null_value} missing values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ba383",
   "metadata": {},
   "source": [
    "The code counts the number of rows in a DataFrame called 'tweets' based on the values in the 'label' column. It then prints the count of each unique value in the 'label' column. The result shows how many rows belong to each label category, providing an overview of the distribution of labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4af94666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    29720\n",
      "1     2242\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the rows based on the label_id column\n",
    "row_counts = tweets['label'].value_counts()\n",
    "# Print the count of each label_id\n",
    "print(row_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2836e25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARoAAAD3CAYAAADCHptSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAvLElEQVR4nO3deXwb1bXA8d/RYtmyHTvO7oREJDEQlhASqKFhKxAKBEpLgbZAWQLtg0ehfS28ug/al/b1tXShtIXH1kJT1gJlacFl37eYELISICaJyb473mTLlnTfH3ecyI73WB7JOt/PZz6WRjN3zkjW0Z07M/eKMQallEomj9sBKKUGP000Sqmk00SjlEo6TTRKqaTTRKOUSjpNNEqppNNE4zIRuVNEftxPZY0XkXoR8TrPXxORK/qjbKe8Z0Xkkv4qrxfb/bmIbBeRzQO9bdU/NNEkkYhUiUijiNSJyC4ReUdErhSR3e+7MeZKY8z/9LCsU7paxhiz1hiTZ4yJ9UPsc0XkgXbln26M+eu+lt3LOPYDfgAcbIwZ3cHrJ4pI3Emw9SKyXkQeFZGjerGNvfY1GQZqO6lIE03ynWWMyQcmADcBPwTu6e+NiIivv8tMEROAHcaYrV0ss9EYkwfkA0cDHwNvisjJAxGg6gFjjE5JmoAq4JR28z4HxIFDnefzgJ87j4cDzwC7gJ3Am9gfg/uddRqBeuA/gRBggMuBtcAbCfN8TnmvAb8E3gNqgH8ARc5rJwLrO4oXOA1oBlqc7S1JKO8K57EHuBH4DNgK3AcUOK+1xnGJE9t24IYu3qcCZ/1tTnk3OuWf4uxz3IljXgfr7rUfzvzbgPcTnv8BWAfUAguB45z5ne3rZcBHQB2wGvi3hLI6/Jyc14qBx519WQNc29V2MmVyPYDBPNFBonHmrwWuch7PY0+i+SVwJ+B3puMA6aishC/zfUAukEPHiWYDcKizzOPAA85re31BE7cBzG1dNuH119iTaOYAnwITgTzgCeD+drH9yYnrcCACTOnkfboPmwTznXVXApd3Fme7dTt8HTgJm6BynecXAcMAH/ZQbDOQ3cW+zgYmAQKcAISB6V19TtjkuBD4CZDlvDergS92tp1MmfTQyR0bgaIO5rcAY4AJxpgWY8ybxvkP7cJcY0yDMaaxk9fvN8YsN8Y0AD8Gzm9tLN5HFwK/M8asNsbUAz8Cvt7uEO6nxphGY8wSYAk24bThxPI14EfGmDpjTBVwM/DNfYxvI/bLXwhgjHnAGLPDGBM1xtwMBIADO1vZGFNujFllrNeBF7AJBTr/nI4CRhhjfmaMaTbGrMYm26/v476kPU007hiLrXK39xtsLeEFEVktImU9KGtdL17/DPsLPLxHUXat2CkvsWwfMCphXuJZojC25tPecOyvf/uyxu5jfGOxtapdACLyAxH5SERqRGQX9nCt0/dBRE4XkfkistNZ/oyE5Tv7nCYAxU7D/y5nvf+i7XuSkTTRDDDnbMhY4K32rzm/6D8wxkwEzgK+n9Cg2VnNprsaz34Jj8djf423Aw1AMCEuLzCiF+VuxH6xEsuOAlu6Wa+97U5M7cva0Mty2vsK8IExpkFEjsM2wp8PDDXGFGLbrMRZts2+ikgAe5j5W2CUs/y/Wpfv4nNaB6wxxhQmTPnGmDM62k4m0UQzQERkiIicCfwNe5y+rINlzhSRySIi2EbLmDOB/QJP7MOmLxKRg0UkCPwM+Luxp79XAtkiMltE/NgG2EDCeluAUOKp+HYeBv5DRPYXkTzgF8Ajxphob4JzYnkU+F8RyReRCcD3gV6fBhZrrIj8N3AFtjYBtu0nim2g9YnIT4AhCau239cs7HuxDYiKyOnAqQnb6exzeg+oFZEfikiOiHhF5NCEU+3dvaeDVsbtsAueFpE67K/dDcDvsGc0OlICvIQ9K/EucLsx5jXntV8CNzpV8ut6sf37sQ3Om4Fs4FoAY0wN8O/An7G1hwZgfcJ6jzl/d4jIBx2Ue69T9hvYsytNwDW9iCvRNc72V2Nreg855fdUsYjUY9+3BcBhwInGmBec158HnsUm18+cWBMPKdvsqzGmDvs+PQpUAxcA/0xYvsPPyUmaZwHTsO/Jduz7W9DRdnqxf2mv9YyGUkoljdZolFJJp4lGKZV0mmiUUkmniUYplXSaaJRSSaeJRimVdJpolFJJp4lGKZV0mmiUUkmniUYplXSaaJRSSaeJRimVdJpolFJJp4lGKZV0mmiUUkmniUYplXSaaFTac3rXS3x+qYjc1s06J4rI53u5naCIPCgiy0RkuYi85XRjmlQiEhKR5T1c9nsicrHz+Dci8rGILBWRJ0Wk0Jk/S0QWOvuxUERO6qCcfyZuU0S+LyIrnLJedrpcRURGiMhz3cWliUZlqhOBXiUa4LvAFmPMYcaYQ7GD97X0d2B95Qx1MwfbFSrAi9iBCqdiuzH9kTN/O3YE1cOwg/zd366cc7DdlCZaBBzplPV34NcAxphtwCYRmdlVbJpo1KAmImeJSIWILBKRl0RklIiEgCuxnasvFpHjnF/mx0VkgTN19MUZQ8LoDMaYT4wxEafG8bGI/NX5xf+70xk8IjJDRF53ag7Pi8gYZ/4kEXnOmf+miBzkzB/l1D6WOFNrMvSKyJ9E5EMReUFEcjqI7yTsyA9RJ74XEjqLnw+Mc+YvMsZsdOZ/iO2kPuBsPw/bOfzPEws2xrxqjAm3L8vxFHacr865PYKdTjrt64QdgWBxwrQWuM15bSh7+sa+ArjZeTwXuC6hjIeAY53H44GPOtjONOzwv+86X8QSZ34IO5TKTOf5vcB12DG03sEOKgd2oLx7nccvJ6xfCrziPH4E+J7z2Ivt2DyEHcVhmjP/UeCiDuL7KXBNJ+/R052scy7wUsLzW7BD1YSA5Z2UdRtwY8LzscCyrj6jwTowvMosjcaYaa1PRORS4Ejn6TjgEacmkYUdnaAjpwAH2xFUABgiIvnGjogAgDFmsYhMxA69cgqwQESOwY4Pvs4Y87az6APYURSeww5H/KJTrhd7mJGHPWx7LGF7rUPdnARc7GwvBtSIyFDseFGLnWUWYhNBe2Ow44W3ISI3YBPVg+3mHwL8ytkfRGQaMNkY8x9OrW8vInIR9r09IWH2VuyAgp3SRKMGu1uxQ/f+U0ROxNZkOuIBjjGdDy0MgLHD/z4BPCEicewIlo+z9+BwBjvg3IfGmGMSXxCRIcCuxOTYA5GExzHsmObtNWKH1Enc1iXAmcDJxql+OPPHAU8CFxtjVjmzjwFmiEgVNjeMFJHXjDEnOuucgh0y6ARjTGI82c62O6VtNGqwK2BPu8olCfPrsAPLtXoB+E7rE+fXvQ0RmenULhCRLOBg9gzlO96p3QB8Azs+1SfAiNb5IuIXkUOMMbXAGhE5z5kvItI6LvnLwFXOfK+TlHrqI2ByQrynYUfo/JLZ076Cc/apHDveeWstDGPMHcaYYmNMCDgWWJmQZI4A7nLK2tpuuwcAXZ4V00SjBru52EOUN7FnW1o9DXyltTEYe6hzpNOYuwLbWNzeJOB1EVmGPQvzPrY2A/ZLfomILAWKgDuMMc3YNpBficgSbPtRa+PuhcDlzvwPgbOd+d8FvuBsYyFwSC/29Vng+ITnt2GT6YvOft7pzP8ONiH92Jm/WERGdlP2b7Bjpz/mLJ84oN4XsImrUzqAnFL7yGnPeMbYU95ux/Ik8J/GmMoB3OYbwNnGmOrOltEajVKDSxm2UXhAiMgIbBtYp0kGtEaj+ihUVp4HTABGA6Ocv4mPRwJBbKOi71TPgsq7s245AHuBW9T524QdE3w9dizstn/n1nTZwKjSh551Ut0KlZVPwF5DcnjrZIyZKAnnZrsVaTBksV+vNjy3YCc26XyMba9YACxkbk1tr8pRrtNEo9oIlZX7gKOB04DjjDFTnbMUbfQmxwDEo825fQinyJmmAuc78wxzC1YCbwOvA28wt6aqD2WrAaSJRhEqK98POM0YcxpwSuIp1d4mlM6Ix9Nf9wQJcKAzzQFgbsFa4BXgMeAF5tZEO11buUITTYYKlZUfAVxgTPxMEU/rfTYuR9Vn44FLnWkHcwueAP4GvMbcmriLcSmHJpoMEiorLzbGXEQ8Nke8vgMBRAbdicdhwLecaTNzCx7DJp13mVujZz5coolmkAuVlXuA002s5Tt4fKeKiAdvxnzso4FrnGktcwseBP6PuTUbul5N9Tc9vT1IhcrKgyYWvRr4vnh9o92OZ1bkpU1/Krh3wK7v6EIL9g7pm5lbs9jlWDLGoKs3Z7pQWXne+P949KcmFt0kXt+vUyHJpBg/cBGwaNN/7/9AqKz8FLcDygQZU4ce7EJl5UPikYYfii9wrSeQm/TuJQeDO6JnTQReDJWVvwP8T9VNs7vtklL1jR46pTknwfxYfIGrxOvry7UqAyKFDp0AWB/OajxW7slGvImn2t4Brq26afZCt+IarPTQKY2Nu/q+q020ea0nkHtdKieZVHRX06wd7ZIM2Dur3wuVld8dKisf7kZcg5XWaNLQ2G/fNd2Tnf+AN1gwxe1YeiqVajTbI97I0bF7fFFPlreLxXYBPwFur7ppdmxgIhu8tEaTRsb9+7zccVf95T5f4ZgF6ZRkUs1fG2Zu7SbJABQCfwQWhcrKT0x6UIOcJpo0Mfbbd1/myc5b5ysY+U3xePVz66PaZon+Kfui3pyJOwx4NVRW/hfnjnXVB/oPm+JGnP3DYeOu+ssr/qKx93qycoa6HU+6+1v99M1Nnjx/H1a9FFu7OaqfQ8oImmhS2Miv/uTM7NARK30FI7/gdiyDQVOU+P9lzRm2D0VMBt4OlZX/yLniWvWQvlkpKFhS6hv9zd/eljNxxlPenPwit+MZLJ6qm7K5xje0o9EDesMP/AJ7/c3YfggrI2iiSTHDvvjv+w096YoF2WOnXC1eX3cNlqqHWuKYW/yX92ZEge6cBCwNlZWf3e2SShNNKhl5zo1fCU45YZl/aPE0t2MZbJ6vCW3d4ivu78bcIuDJUFn5j7pdMsPpLQgpIFhS6s877JSf5Ew66ofi9feloVJ1IW7gZt+crCQVL8AvQmXlE4Grqm6arZ1udUBrNC4LlpTm5U07/eGcyUffoEkmOd6sHbV9jX9yss/YXQGUh8rK87tdMgNponFRsKR0xJDSc58LTjrqq+LxpG33dqnut20GqEyqU4G3QmXl4wZqg+lCE41Lcqccv3/BzAteyR538Ey3YxnMFtYWVi8LTBvI+5amAvNDZeXTBnCbKU8TjQvyps46tGDm118KjJ7s+siGg90t8W9Eul+q340FXg+VlZe6sO2UpIlmgOVOOf7IIZ875+ms4RMmuh3LYPdxfbDurezj3Or4awjwvF5JbGmiGUDBktKjh5R+9aGs4eNDbseSCf4YPafO5RAKgBdCZeVHuhyH6zTRDJBgSenhQz731T8HRk8ucTuWTPBZOBAuD5xW7HYc2LvAnwuVlR/sdiBu0kQzAIIlpQfmHzH7nuz9DjnE7VgyxZ2R03am0FAyw7A1m5DbgbglZT6JwSpYUhrKPfTkP+VMnDHD7VgyxdYmX+SR7HNSoTaTaCz2/qiRbgfiBk00SRQsKS0OHjDzruABnz/W7VgyyT2NJ2yLiz8V/7cnA4+Hysoz7sLMVPwwBoVgSemIrNGTf517yIknSxqPNZtudjV7WuYFLkjlIWaOBX7vdhADTRNNEgRLSoOenCFlQ2acfZZ4vHoHNlC9Y4d32p31tE5DflnL7+e3vcTFGMO1zzYx+Y91TL2jng822a56tzXEOfbeBg69vZ6nPm7ZvfzZfwuzsa7t0NoPNRy1JeLJSfV7+P49VFZ+mdtBDCRNNP0sWFLqAbmk4OjzzvVk5/ZntwRpbeiwYbHFV+ax+Mo8Fn47l6Bf+MpBbY8gnv00SuXOGJXX5HH3WdlcVd4IwMPLW7jkcD/vXp7Lb95pBuDpT1qYPtpLcf6ef+FwVGJ3ZF2SLqMX3JFJ19hooul/J+dNO/1b/qKx490OJFW9vCbGpCIPEwrb/vv94+MoF0/NQkQ4epyPXU2wqS6O3yM0Rg2RmMEjEI0bfl/RzPUz296Q/XjdIVvqvIXZA7kv+yAAPJEpjcOaaPpRsKS0JDD+sOtyJk6f5nYsqexvy1v4xqF7t4duqDPsV7CnOWvcEGFDneGCw/w8vyrGaQ+EmXtCgNsXNHPxVD9B/55lm2PE/+C/vGBAdqD/jAMeC5WVp/qh3j4b9Ds4UIIlpYXeISPK8qedcZyI3ondmeaY4Z+fRPnlyYG9XutohDEBCrKF8guCAFQ3Gn71doQnvhbkW/9spLrJ8INjstiad+DW7TmjUrkRuDPHAzcAP3U7kGTSGk0/CJaU+oBvFxx1zhc9/sC+9kk7qD1bGWX6GA+j8vb+1xuXL6yr2ZNu1tcaivPb5uyfvR7hhuMCPLyshRnFXu49O4cfvRzht7456fy+3xAqKz/M7SCSSRNN/zgreNCxX/IVjtLOqrvxcCeHTQBfOtDHfUubMcYwf32UggCMSWjsrdwRY2N9nBNCPsIttr1GgM1N/ug6//7pdtiUyA/8JVRWPmjPUGqi2UfBktIJnmDBebkHHjvd7VhSXbjF8OLqGOdM2ZNo7ny/mTvft2eSzijxMbHQw+Rb6/nW003cPrttJeWGVyL8/Av2kOsbh/mZt7iFo+9pIHvGOY0DtxdJMwO43u0gkkXH3t4HziHTjYXHX/KNrBETDnA7nlSWrLG3K2qG7fxa4NbBMiRNBDii6qbZH7kdSH/TGs2+OTF7wrSZmmTc8zsuaOl+qbQRAO4djIPTDbodGijBktKR4s++MG/qrIy56CrVfFiXV1MROGaU23H0s6OBa9wOor9poumDYEmpABflTz9zuicrJ50bIdPa72PnNrgdQ5L8d6isvNDtIPqTJpq++ZyvcMzMwNiDtM9fl6xpyG54MfvUVOsKor8MBX7odhD9SRNNLwVLSnOBb+ZPO22KiEffP5f8X/PsardjSLLvhsrKB00i1S9K753kHzlxnK9o3BS3A8lUmxr9TY9nf3mwX7OUA/y320H0F000vRAsKS0EvpR/2KxDtIsZ9/yp6aTtRryZ8AHMCZWVD4ozmppoeuf0rNElI3yFoya7HUimqo54mu8PfL3fr8dJUT7gf90Ooj9ooumhYEnpMOCUvENOyuje7N12X/iYrS2ewKC9VL8D54bKytO+v2lNND13ataoSUW+wlE6XIpL6lskdlfWxRnRf0s717kdwL7SRNMDrbWZ4EHH6eiSLnqs/vDNYW9+VvdLDjrnhsrKx7kdxL7QRNMzJ3my87P8RWP1sMklkRjxW7PmDHU7Dpf4gO+4HcS+0ETTjWBJaTZwcu6U40eLx6sdhbnkmdoDtuz0Dg+6HYeLvhUqK0+Xbkr3oomme1OBQKD4oGluB5KponHMzb45uW7H4bIi4Hy3g+grTTRdcO5pOj17/NQ8T3ZuuvSuP+i8XLvf1o3+8TqiBPyb2wH0lSaaro0DQjmTjjrI7UAyVdzAbz2XZdzIjp34fLp2+amJpmvHeYKFPl/hGL3dwCXv1o7YUZl10GDp2Ko/XOh2AH2hiaYTwZLSHOCE4AHHjBCPJ5MuEEspN/PNePdLZZSvuB1AX2ii6dw0wJ81Yn+9QM8lS2qH7PogcOQIt+NIMQeEysoPcTuI3tJE07njPIHcZm9e0QS3A8lUv4+fPxg6HU+Gc9wOoLc00XTA6XPmwOyJM4bpYZM7Pm3IqX81+6RMuXmyt9Lu8EkTTcdKAAmMmjQobtFPR7c2f6nG7RhS2BGhsvKQ20H0hiaajk1HPC2+Ar2B0g3rw1mN/8g+c9D0LpckaVWr0UTTjjNW05HZ+x2aLb6sdB5mNW3dHZm1g8zo2GpffMntAHpDE83eQkAgMHbKJLcDyUQ7It7mhwLnadtM90pDZeVpc++dJpq9HQoYX8HI8W4HkonmNczcEvVkaQN893KAw90Ooqc00eytFPHs8uQM0V/VAVbbLNE/ZV802u040sgxbgfQU5poEgRLSoPAqKzRk3O1S4iB90j99C1Nnjy9r6nnjnY7gJ7SRNNWMRDPGj5Bz3gMsKYo8duy5ug9Tb2jNZo0NRbw+ApHa6IZYE/VTdlc4xuqZ/l6Z2KorDwtbtHQRNPWQUCjN69IE80AaoljbvFfrv3N9E1aHD5pomnrALz+Bk92/ii3A8kkz9eEtm7xFee5HUeamu52AD2hDZ6OYElpPlCUNWpiRO9vGjhxAzf75mTiyAb9ZX+3A+gJrdHsUQzEfQWjtEFyAL1ZO2r7Gv/kTB3doD+E3A6gJzTR7DEK8Hhzhxa6HUgm+S2XuB1CutMaTZoZA7R4c/IL3Q4kUyysLaxeFpimnb7vm7HpcCuCJpo9xgCNnkBeoduBZIpb4t+IuB3DIOAFUv52GU00e4wGmiQrO9/tQDLBx/XBureyj9PbDfpHyO0AuqOJht3jNw0HIuIL6GnWAfDH6Dl1bscwiKR8O40mGisH8CBixJeV6SMiJt1n4UC4PHCaXhTZf1K+ZqiJxsoD4t7coTkioh0uJdmdkdN2iui/Xj9K+cN9/bStPADxaj8oyba1yRd5JPscrc30L000aSIHELw+TTRJYozxAtzTeMK2uPj1/65/aaJJE17AiMer70eSmFgse1ezp2Ve4IKUb09IQyl/17t+sSwPIKI1mqSJG5P1UMNRWyKenJS/uCwNpXxnYZpoLJtgPD59P5Jkl6cwdkfWJXoVcHKkfKLRXxfLqdF4tUaTJIuCx+hlA8mT8t9j/QW3PADaRqPSVMztALqjXyxLazIqnaX88MGaaCwPIPGmhia3A1GqD2rdDqA7mmisOECssabR7UCU6gOt0aSJRiAea9iliUalI63RpIlGwBCPxU0sqn2kqHSjNZo0sbsmY2ItWqtR6UZrNGmiERAAE23WRKPSjdZo0kSYPYkm7HIsSvXWBrcD6I4mGmt3jSbe0lTvcixK9dZKtwPojiYaIFxZEQOaAF+8oXq72/Eo1VPGmK1VN83WQ6c0sh0IRGu2bnM7EKV6SkRSvjYDmmgSVQG5LTs3aKJR6UQTTZqpArJbdqytNvFY1O1glOohTTRpZgtgMMbEIw3aTqPShSaaNLP7kCneWKuHTypdfOJ2AD2hiWaPHc5fidVXa6JRKc8YUwt87HYcPaGJxhGurIhiD59yWqo3bnI7HqW6IyJvVd00O+52HD2hiaatKiA3sn7FOmOMcTsYpbrxutsB9JQmmrY+BnLiTXWReGPtZreDUaobb7gdQE9pommrCjAA0ZqtVa5GolQXjDFh4H234+gpTTRtbQSigK95W9Uat4NRqgvvVN00O22u99JEk8BpEP4EKGj6bEmVicdTvnd5lZlEJG3aZ0ATTUcWArmmOdwSa6he63YwSnXiVbcD6A1NNHv7tPVBy871q9wMRKmOGBPfBrzrdhy9oYlmb5uAeiDQtOaDFW4Ho9Te5PF0uX6mlSaadsKVFXFgATCsZce66lhD9Tq3Y1IqkYg87nYMvaWJpmMLcMYzjmyqXOpyLErtZuLxncBrbsfRW5poOlaJPXzKDlfO/1DPPqmUITycTqe1W2mi6YDTtefrwPB4eFdjtHbrp92to9RAEPHc53YMfaGJpnPvAV6AyIaPlrgci1KYeHRN1U2z33M7jr7QRNO59cBmIL/x04qVJtbS5HZAKsOJ53a3Q+grTTSdCFdWGOAlYKiJNsdatq9b5nZMKnOZeKxRxHO323H0lSaari3GjvckDStee0e7jlBuMbGWeVU3zU75oW87o4mmC+HKih3AUmBEy871u6LVGz90OyaVeYwxcY8/+1dux7EvNNF07xkgCNDw8VtvuRyLykCmJfKvqptmf+Z2HPtCE033PgVWAUXNmz7ZEq3dpqe61YASX9bP3I5hX2mi6YbTKPwPYIh9Pl9rNWrAxFsiH3z267MWuB3HvtJE0zPLsae6hzRVLfos1lC93u2AVGYQj/fHbsfQHzTR9IBzo+UTQBFAeNX7adNXq0pf8UjDe5/95ux/uR1Hf9BE03OLgF1AsLHy3cpo7Tbtq0YljTFxY6LNV7kdR3/RRNND4cqKFuBJYCRA3ZLnnzMmnlZ9gqj0EQ/XPrXu1os+cDuO/qKJpnfexnZgPrRl6+rtzVtWpeV9Jyq1mVg0Ir6sQVObAU00veJ0Xn4fUAhI3QfPvBaPNje4G5UabGKNtbetveW8LW7H0Z800fTeJ0AFMCbeWBdpWvPBy24HpAaPeEvTTvH6bnQ7jv6miaaXnOtqHsO+d/76ZS8ujoVrNroclhok4uHa7637wzcGXU8BovcJ9k2wpPQs4CvA2sC4Q4qHfO6cK0RE3I6rL2rf/wf1S54HA3mHf5EhR53NrjfuJ/xpBYjgDRYy7Izv4csftte6jasXsvPluyEeJ+/wUyk4+jwAql/7C42rF5I1cn+Gn/kDAOqXv0K8qY4hR549oPuXLqK1W19Zf/tlJ7sdRzJojabvXgRqgPzI+g83RjZ+/KbbAfVF87Yq6pc8z+iLf8eYObfSuOo9WnZuYEjpVymecxvFl91KzqSjqHnn4b3WNfEYO1+8g5Hn/ZTiK26nYcXrNG9fSzzSQGTDRxTPuQ1j4jRvqyLeEqFh+UvkHzHbhb1MffFIuLZ5y6oL3I4jWTTR9FG4sqIJmAeMADy1C558PRau3eRuVL3XsmM9geKD8PizEY+XwH6HEq58F08guHsZ09KE7S2jreZNK/EVjsFfOBrx+smdcjyNlfMBwcSiGGMw0WbE46X2vSfIn/ElxOsbuJ1LE8YY07yt6uqtj/98UDUAJ9JEs2+WYnukH0ssGq9b+M8nTDyWVh1HZw2fQNO65cQaa4m3NNG4+n1itdsBqH7jPtbffikNK16j8LiL9lo3WrcD35ARu59784cTq9+BJxAkeODn2TTvWnwFo5BALs2bVhIsOXrA9iudRHdueGbzA9c/4HYcyaRtNPsoWFKaC/wMm7Rr8g7/4ozg5NIzXQ6rV+qWvED9onLEn41/+H6IL0DRyd/a/XrNu49ioi0UHndhm/UaPn6LpjUfMOz0awHbBtO8aSVFs65ss9yOZ/9I/vTZRDZ/StOaRfhHhij8/NeTv2NpINZYt62p6oNJ2/7x6zq3Y0kmrdHso3BlRQNwNzAU8NUveX5hS5p1kJV/+KmMufQPjL7wV3iy8/EPLW7zeu7BJxJe+fZe6/nyhxGt3bb7eaxuO968ojbLNG+xd2r4ho6lYfkrjPhyGS3bPqNl54Yk7El6MfFYrGX7Z5cM9iQDmmj6Rbiy4hPsTZfjAGrefeTpeCRc7W5UPRdr2AVAtHYr4ZXvEjz4hDaJIPxpBf6icXutlzXmAKLVG2nZtRkTa6HhozfImVzaZpldbz5AwbEXQjwKxrljQzyYaCRp+5MuIptW3rr5wR8+63YcA0Fb5vrPv4CDgVC8sW5z7YInHy445vzLxesPuB1Yd7Y99QvijXXg8VI060q82XnsfPaPtOxcD+LBN2QERV+8GrDtMjue+yOjzvsp4iy/9dGfgImTd9gsskZM2F1ueOW7ZI0u2X1aPFB8EBvvuRr/yBBZIye6sq+ponnL6vm18x+7Hq5zO5QBoW00/ShYUjoM214TAWpzJh65f9600y4S8WjNUe0Wrd22vva9J6bXvv/Pbd0vPTjoF6AfOZ2Z/wHbXpPduPr9NY2VFU+7HJZKIbHGupqGFa+fnUlJBjTR9LtwZcVK4C6gGPDVL3txcWRDel7Mp/pXvCUSaVjx2pXVr/1l0HT/0FOaaJIgXFkxH3gUGA9IzfxHX2nZuWG5y2EpF5l4LBb++M2fN1UtesTtWNygiSZ5/oW9mG8CwK43738qVr9zrasRKVeYeCzWsOK1O8Ir3/mlc1NuxtFEkyROP8MPACuAsSbaHKt+fd6Dmmwyi4lHY/VLX3wg/Mnb14crK2Jux+MWTTRJFK6saAZuB7YBo+NN9c07X733gWjdjjUuh6YGgIlHY/VLXniocdV71zj3xmUsPb09AIIlpUOB64FhwCbxB3xDT7jsa76CkZNdDk0liZNkHmxc/f53wpUVg/7K3+5oohkgwZLSAuzVWaOBDeL1ewtPvOxcf+Hog1wOTfUzJ8k80Lj6/Ws0yViaaAZQsKQ0H/g+sB+wHo/XM/SES8/xF409xOXQVD8x0ebmuqUvPtS0ZuG1mmT20EQzwJy7vb8LTALWIR4pPPaC07NGTjzK5dDUPoo11tXULnjyvpZtVf8VrqyodzueVKKJxgXBktIc4BrgIGAdYPKmnjo9Z9LnzhCPx+tudKovWnZt3lTz7qP3xMO7fqVJZm+aaFwSLCnNBq4AjsImm2hgv8PG5R9xxtc8/kCeu9Gp3mja8NHHtQue/B2x6H3hygq9Lb0DmmhcFCwp9QJnAOcCW4EG75CReYWf/9rXvLlD9+6XQaUUY+Lx8Cdvv9fw4as/A57L1IvxekITTQoIlpQeDlwNNAPbxev3Fsy84IysEROmuxya6kS8uamhbvGzr0bWLbshXFmx1O14Up0mmhQRLCktxjYSDwPWA+QdNuuInElHnSZeX5arwak2mrd/tqp2wVPPxcM1N4UrK9a7HU860ESTQoIlpXnYdpsjcNpt/EXjCvOP+vKXfXlFE7peWyWbiTY31a947b3GyvnPAHeHKytq3I4pXWiiSTEJ7TbnAHXATkQkf/qZR2ePn3qSeHS8Eje07Nq0trbi8bdj9Tv/DLyeyfct9YUmmhQVLCndH/g29kri9UDMP2z80PwZZ57lyx++v7vRZQ4Ti7aEK999v+HDV58D/hyurNDhj/tAE00Kc06Bnw2cDtQCOwHyDps1LWfijFniywp2tb7aN83b11bWLSpfFKvddj/wfLiyosXtmNKVJpo0ECwpnQhcDowFNgAtnuz8QN600z4fGFNyjHh8fncjHFxi4ZrN9Utf+CCy4aOlwF3hyooqt2NKd5po0kSwpDQLmIVtu4kDm4C4d8iIvPzDTzvBP2LCdO0Efd/EmxtrwpXzF4U/eetTjHkKeEkvwOsfmmjSTLCkdDjwJeB4oBF7oZ/xD59QlDd11sn+ocUHuxpgGjLR5nBj1eJF9ctfXk2s5S3gcaejedVPNNGkqWBJ6X7AV7GnwmuBHQCBcYcUBw+cebyvYNQBIiJuxpjq4k0NO5rWLl3S8NHrm0y0eQXwSLiyYpXbcQ1GmmjSWLCkVIAS4GvYu8GrgRoAX9G4wtyDjj0qa8T+08Xnz3YxzJQTrdtR1bjqvWWNqxbswB6CPgws11sIkkcTzSAQLCn1AFOx90yNxQ5gtxWIiz/blzvl+KmBcYd8zpuTP8rNON1kTDzesnPD8vBHb6xs3rKqAfgUeBqbYPSamCTTRDOIODWcydhG4yMBg004EYDs0BETcvY/YoavYPSBmXBbgzGGeEP12siWVR83Vs7fFmuobgbmAy8CVVqDGTiaaAYpZ3jemcAXgSD2kGoXgPiyvDkTZ0zKGnPgFH/h6APFl5XjXqT9L9awa33z1tUfhj+t2BCr3ebBJtoXgDfClRXbXQ4vI2miGeSCJaUB4HDsRX+t90vVYhOPweP15ISOmBAYe9AU39CxU9KxLxwTj8di4V0bWravXdm46r110V2bW1/6BHgdWBqurAi7F6HSRJNBnFPjhwDHYhuPAeqxNZ04QNbokpFZoydP8A8tnuDNHzbe48/OdyXYLphYtDnWUL0uumvzZ81b16yNrF9RZ2LN+dhDxU+BN7BtL7tcDVTtpokmQwVLSguBKdikMwX7JY1jazq7u6L0FY4ekjVqcrG/aOxYb/6wYk8gt0j82QUDdercxKLN8Uj9tli4dnusbvvm5i2r1kU2ftKAiecB4kxrsMllWbiyYudAxKV6RxONah2dYRJwAPYwaww28QjQgL2LvHn3Ch6vxz+0uMBXMKrQmzdsqDe3YKgnO7/QEwgW4PEFxOP14/H4xOP1I15/+36QTTweIx6NmFg0Ypy/xKLNJhppjDXWVcfDu3ZF63ZUR6s37YzWbI4Aec7U+s+6GliCrb2s1cOi1KeJRu3F6RdnHHZYmCnYa3XygBg2+XiAFqAJe3VyBOfQq0MiIllBv3h9HhMJt5hYS+LpZC/gAwJAjvMX9iS6OuAzYBm25rJObwtIP5poVLec0+a5QCEw1PlbjK35jMH2CuhlT7Lp7p9KEqZmIIwdNnijM+0AtgPbtLYyOHSbaETEAL8zxvzAeX4dkGeMmdvFOl8GVhpjVnTw2lyg3hjz24R5VcCRxphOTz2KyH8ZY37RZbB7r3Mm8D/YX2A/8AdjzF29KaMvRGQe8Iwx5u/dLJcDPAecBBwG3AEMwdYc/tcY84iz3EnAb4EsYCFwuTEmKiLXAxc6xfmwtY8R2KRwH7YvmzhwtzHmD05ZRcAjQAioAs43xlSLyGHAD4wxl/Z2f50LBv0Jk6+D51Fszad1agYi4cqKzmtCavAwxnQ5YavHa4DhzvPrgLndrDMPOLeT1+YC17WbV9Vafhdl1ncXa7vl/dhfx3HO8wBwYG/K6OvU1f63W+5q4LvO4wOAEudxMfbS+EJsklwHHOC89jNsomlf1lnAK87jMcB053E+sBI42Hn+a6DMeVwG/CqhjJeA8QPxHumUWVNPuhWIAncD/9H+BRGZICIvi8hS5+94Efk89u7i34jIYhGZ1H69rojIUyKyUEQ+FJFvO/NuAnKc8h505l0kIu858+4SkfYDr+Vjf0l3ABhjIsaYT5x154nInSLypoisdGo+iIhXRH4jIgucffq3hLiuT5j/04T5FzvzlojI/QnbP15E3hGR1SJybie7eyHwDye+lcaYSufxRuwVvSOwhyURY8xKZ50XsTdTtvcN7D07GGM2GWM+cB7XAR9hb00A25HWX53HfwW+nFDG08DXO4lVqb7rLhNhT3UOwdY6Wgeqn+u89jRwifN4DvCU83geXddoNgCLE6Zm9tSYipy/OcByYFhrHAllTHG27Xee3w5c3MG2/oz9wj6M/VJ7EuJ7DltbKMF2lZmN7TrzRmeZAPA+sD9wKjbZtjaEPoPtpuEQ7EVh7WOfBzzmLHsw8GkHsWUBmzt5jz6HTQ4eZ5ufYQ8tAf4ALGu3fBDb+15RB2WFgLXAEOf5rnavVyc8ngk87favn06Db+pRR9fGmFoRuQ+4FnuWodUx2I6YAO7HVst74hazdxtNq2tF5CvO4/2wiaB93yAnAzOABc7lHDnYhNI+7iuctodTsAlyFnCp8/Kjxpg4UCkiq7HD054KTE2ogRQ42z/VmRY58/Oc+YcDfzdO25IxJvEajqec8leISEc3Mw7HuSUgkYiMwb6XlzjrIyJfB24RkQD2Uvpou9XOAt5ut31EJA94HPieMaa2gxja24o9bFOqX/WmR/3fAx8Af+limX06hSUiJ2KTwjHGmLCIvIataey1KPBXY8yPuivTGLMMWOYc1qxhT6JpH2vr6dRrjDHPt4vri8AvTbuGZBG5toNyWiWegu3o4rZG2u2biAwByrG1qvkJ+/AucJyzzKnY9pxEX8c5bEooy49NMg8aY55IeGmLiIwxxmxyklpigs6m7Q+JUv2ix10/Or+Wj2L7rm31DnuO6S8E3nIe12HbSHqrAFuVD4vIQcDRCa+1OF8egJeBc0VkJNgzKSLSZtwjEclzEleradhDkFbniYjHaUOaiD0Eeh64qnU7InKAiOQ68+c4NQREZKyz7ZeB80VkWGscPd1RY0w14BWRbGfdLOBJ4D5jzGPt9qV1PwPAD4E7E14rAE7Aaetx5glwD/CRMeZ37Tb9T+AS5/EliethE9jynu6DUj3V2z5mb8ZW+VtdC1wmIkuBb2JHWgT4G3C9iCzqZWPwc4DPKe9/sLf0t7obWCoiDxp72vxG4AVn2RexZ1oSCfCfIvKJiCwGfsqe2gzsueHuWeBKY0wTtk1nBfCBiCwH7gJ8xpgXgIeAd0VkGfB3IN8Y8yHwv8DrIrIEaP+l7s4L2FsAAM7Htvtc6jRwLxaRac5r14vIR8BSbBvKKwllfAV4wRjTkDBvJvbzOCmhrDOc124CZolIJfZQ8qaE9b6ArVEp1a8y8oK9nl7nMgBxHAF83xjzTTfjcGIJYBPvscaY9m1ASu0T7TXfRcaYRcCrHZyad8N47PU1mmRUv8vIGo1SamBpjUYplXSaaJRSSaeJRimVdJpolFJJp4lGKZV0mmiUUkmniUYplXSaaJRSSaeJRimVdJpolFJJp4lGKZV0mmiUUkmniUYplXSaaJRSSaeJRimVdJpolFJJ9/95e3BF6q3FfwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "# Calculate the label counts\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "# Prepare the data for the pie chart\n",
    "labels = ['Not Hate Speech ({})'.format(label_counts[0]), 'Hate Speech ({})'.format(label_counts[1])]\n",
    "sizes = [label_counts[0], label_counts[1]]\n",
    "\n",
    "# Create the pie chart\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "\n",
    "# Add a title and display\n",
    "ax.set_title(\"Distribution of Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ee447f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "# # Extract the text column\n",
    "# text_data = tweets['tweet'].astype(str)\n",
    "\n",
    "# # Concatenate all the text data into a single string\n",
    "# text = ' '.join(text_data)\n",
    "\n",
    "# # Generate word cloud\n",
    "# wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# # Create a plot and display the word cloud\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "print(\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548285b",
   "metadata": {},
   "source": [
    "# Phase 2 - Data Cleaning and Normalisation\n",
    "\n",
    "In this phase, we focus on preparing the tweet dataset for further analysis by performing data cleaning and normalization. The codes below outline the steps involved in this phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0df1065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex expression, stopword and POS tagging\n",
    "regex = r'@user|#\\w+|[\\.!&’?/♂️:,-]|\\b\\w{1,2}\\b|\\d+|[^\\w\\s]+'\n",
    "\n",
    "# Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# POS tagging function\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return 'n'  # Default to noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c81bfb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lemmatized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>father dysfunctional selfish drag kid dysfunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks credit use cause offer wheelchair van pdx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>love take time urð</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                    lemmatized_tweet  \n",
       "0  father dysfunctional selfish drag kid dysfunction  \n",
       "1   thanks credit use cause offer wheelchair van pdx  \n",
       "2                                     bihday majesty  \n",
       "3                                 love take time urð  \n",
       "4                                 factsguide society  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatized_tweet(word):\n",
    "    word = re.sub(regex, '', word, flags=re.MULTILINE)  # Apply regex\n",
    "    word = word.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(word)  # Tokenize the text\n",
    "    tagged_tokens = pos_tag(tokens)  # Perform POS tagging\n",
    "    lemmatized_tokens = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        if token not in stop_words:\n",
    "            pos_lemma = get_wordnet_pos(pos)\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, pos=pos_lemma)\n",
    "            lemmatized_tokens.append(lemmatized_token)\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "tweets['lemmatized_tweet'] = tweets['tweet'].apply(lemmatized_tweet)\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29d795f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lemmatized_tweet</th>\n",
       "      <th>stemmed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>father dysfunctional selfish drag kid dysfunction</td>\n",
       "      <td>father dysfunct selfish drag kid dysfunct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks credit use cause offer wheelchair van pdx</td>\n",
       "      <td>thank credit use caus offer wheelchair van pdx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday majesty</td>\n",
       "      <td>bihday majesti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>love take time urð</td>\n",
       "      <td>love take time urð</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society</td>\n",
       "      <td>factsguid societi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                    lemmatized_tweet  \\\n",
       "0  father dysfunctional selfish drag kid dysfunction   \n",
       "1   thanks credit use cause offer wheelchair van pdx   \n",
       "2                                     bihday majesty   \n",
       "3                                 love take time urð   \n",
       "4                                 factsguide society   \n",
       "\n",
       "                                    stemmed_tweet  \n",
       "0       father dysfunct selfish drag kid dysfunct  \n",
       "1  thank credit use caus offer wheelchair van pdx  \n",
       "2                                  bihday majesti  \n",
       "3                              love take time urð  \n",
       "4                               factsguid societi  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize stemming (this was included for comparison, although, lemmatized text would be adopted)\n",
    "stemmer = PorterStemmer()\n",
    "def stemmed_text(word):\n",
    "    word = re.sub(regex, '', word, flags=re.MULTILINE) # Apply regex\n",
    "    word = word.lower() # Convert to lower case\n",
    "    tokens = word_tokenize(word) # Tokenize the text\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words] # stem the tokens\n",
    "    return \" \".join(tokens)\n",
    "tweets['stemmed_tweet'] = tweets['tweet'].apply(stemmed_text)\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a273089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "# # Extract the text column\n",
    "# text_data = tweets['stemmed_tweet'].astype(str)\n",
    "# text = ' '.join(text_data) # Concatenate all the text data into a single string\n",
    "# wordcloud = WordCloud().generate(text) # Generate word cloud\n",
    "# print(\"WordCloud Stemmed Tweets\")\n",
    "\n",
    "# # Create a plot and display the word cloud\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "print(\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56753fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "# # Extract the text column\n",
    "# text_data = tweets['lemmatized_tweet'].astype(str)\n",
    "# text = ' '.join(text_data) # Concatenate all the text data into a single string\n",
    "# wordcloud = WordCloud().generate(text) # Generate word cloud\n",
    "# print(\"WordCloud Lemmatized Tweets\")\n",
    "\n",
    "# # Create a plot and display the word cloud\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "print(\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b527441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "train_set, test_set = train_test_split(tweets, test_size=0.2, random_state=42)\n",
    "# Save the training set to a CSV file\n",
    "train_set.to_csv('train_set.csv', index=False)\n",
    "# Save the test set to a CSV file\n",
    "test_set.to_csv('test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba041d",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "In this phase, we will train several machine learning models using the cleaned and normalized tweet dataset. The code below outlines the steps involved in this phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd380ca1",
   "metadata": {},
   "source": [
    "### Approach 1. Upsample then Vectorisation (TFIDF & WORD2VEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5df421c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>lemmatized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>pay attention past year year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>raft building salford quays gmw fun outdoors badge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>gracious muse jewelry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                    lemmatized_tweet\n",
       "0      1                        pay attention past year year\n",
       "1      0  raft building salford quays gmw fun outdoors badge\n",
       "2      0                                              friday\n",
       "3      0                                                true\n",
       "4      0                               gracious muse jewelry"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in train dataset\n",
    "train_tweets = pd.read_csv('train_set.csv')\n",
    "columns = ['label', 'lemmatized_tweet']\n",
    "train_tweets[columns].to_csv('train_tweets.csv', index=False)\n",
    "train_tweets = pd.read_csv('train_tweets.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "train_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17ef837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    23783\n",
      "1     1786\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the rows based on the label_id column\n",
    "train_tweets_row_counts = train_tweets['label'].value_counts()\n",
    "# Print the count of each label_id\n",
    "print(train_tweets_row_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78a81f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 missing values.\n"
     ]
    }
   ],
   "source": [
    "#Check for missing values\n",
    "null_value2 = train_tweets_row_counts.isnull().sum().sum()\n",
    "print(f'There are {null_value2} missing values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8a73fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    23783\n",
      "0    23783\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate the majority and minority classes\n",
    "majority_class = train_tweets[train_tweets['label'] == 0]\n",
    "minority_class = train_tweets[train_tweets['label'] == 1]\n",
    "# Upsample the minority class\n",
    "upsampled_minority = resample(minority_class,\n",
    "                              replace=True,  # Sample with replacement\n",
    "                              n_samples=len(majority_class),  # Match the number of instances in the majority class\n",
    "                              random_state=42)\n",
    "# Combine the upsampled minority class with the majority class\n",
    "upsampled_train_tweets = pd.concat([majority_class, upsampled_minority])\n",
    "# Shuffle the upsampled dataset\n",
    "upsampled_train_tweets = upsampled_train_tweets.sample(frac=1, random_state=42)\n",
    "# Print the class distribution after upsampling\n",
    "print(upsampled_train_tweets['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dadb0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = pd.read_csv('test_set.csv')\n",
    "test_data = test_tweets[['label', 'lemmatized_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc03f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline creation using TF-IDF vectoriser and five classifiers\n",
    "\n",
    "def TFIDF_pipeline(classifier):\n",
    "    # Define the pipeline steps\n",
    "    steps = [\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('classifier', classifier)\n",
    "    ]\n",
    "    # Create the NLP pipeline\n",
    "    TFIDF_nlp_pipeline = Pipeline(steps)\n",
    "    return TFIDF_nlp_pipeline\n",
    "\n",
    "# Instantiate the classifiers\n",
    "NB_TFIDF = MultinomialNB()\n",
    "LR_TFIDF = LogisticRegression()\n",
    "SVM_TFIDF = SVC()\n",
    "RF_TFIDF = RandomForestClassifier()\n",
    "DT_TFIDF = DecisionTreeClassifier()\n",
    "\n",
    "# Create the NLP pipelines for NB, LR, SVM, RF, DT using TFIDF text representation\n",
    "TFIDF_NB_pipeline = TFIDF_pipeline(NB_TFIDF)\n",
    "TFIDF_LR_pipeline = TFIDF_pipeline(LR_TFIDF)\n",
    "TFIDF_SVM_pipeline = TFIDF_pipeline(SVM_TFIDF)\n",
    "TFIDF_RF_pipeline = TFIDF_pipeline(RF_TFIDF)\n",
    "TFIDF_DT_pipeline = TFIDF_pipeline(DT_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11f26a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of Word2Vec model\n",
    "sentences = [tweet.split() for tweet in tweets['lemmatized_tweet']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=multiprocessing.cpu_count())\n",
    "#Custom vectorizer for Word2Vec\n",
    "class Word2VecVectorizer:\n",
    "    def __init__(self, word2vec_model):\n",
    "        self.word2vec_model = word2vec_model\n",
    "    def transform(self, X):\n",
    "        return np.array([self.get_word2vec_embedding(text) for text in X])\n",
    "    def get_word2vec_embedding(self, text):\n",
    "        words = text.split()\n",
    "        embeddings = []\n",
    "        for word in words:\n",
    "            if word in self.word2vec_model.wv:\n",
    "                embeddings.append(self.word2vec_model.wv[word])\n",
    "        if embeddings:\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.word2vec_model.vector_size)\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "# NLP Pipeline with Word2Vec\n",
    "pipeline_lr = Pipeline([\n",
    "    ('word2vec', Word2VecVectorizer(word2vec_model)),  # Custom Word2Vec vectorizer\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "pipeline_dt = Pipeline([\n",
    "    ('word2vec', Word2VecVectorizer(word2vec_model)),  # Custom Word2Vec vectorizer\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "pipeline_rf = Pipeline([\n",
    "    ('word2vec', Word2VecVectorizer(word2vec_model)),  # Custom Word2Vec vectorizer\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "pipeline_svm = Pipeline([\n",
    "    ('word2vec', Word2VecVectorizer(word2vec_model)),  # Custom Word2Vec vectorizer\n",
    "    ('classifier', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "441d285b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\AppData\\Local\\Temp\\ipykernel_18912\\853931414.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['lemmatized_tweet'].fillna('', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NB Classification Report (Test Dataset):\n",
      "NB Accuracy (Test Dataset): 0.8681\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93      5937\n",
      "           1       0.32      0.73      0.44       456\n",
      "\n",
      "    accuracy                           0.87      6393\n",
      "   macro avg       0.65      0.81      0.68      6393\n",
      "weighted avg       0.93      0.87      0.89      6393\n",
      "\n",
      "\n",
      "LR Classification Report (Test Dataset):\n",
      "LR Accuracy (Test Dataset): 0.9022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95      5937\n",
      "           1       0.39      0.68      0.50       456\n",
      "\n",
      "    accuracy                           0.90      6393\n",
      "   macro avg       0.68      0.80      0.72      6393\n",
      "weighted avg       0.93      0.90      0.91      6393\n",
      "\n",
      "\n",
      "SVM Classification Report (Test Dataset):\n",
      "SVM Accuracy (Test Dataset): 0.9496\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      5937\n",
      "           1       0.73      0.47      0.57       456\n",
      "\n",
      "    accuracy                           0.95      6393\n",
      "   macro avg       0.84      0.73      0.77      6393\n",
      "weighted avg       0.94      0.95      0.94      6393\n",
      "\n",
      "\n",
      "DT Classification Report (Test Dataset):\n",
      "DT Accuracy (Test Dataset): 0.9065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      5937\n",
      "           1       0.40      0.61      0.48       456\n",
      "\n",
      "    accuracy                           0.91      6393\n",
      "   macro avg       0.68      0.77      0.71      6393\n",
      "weighted avg       0.93      0.91      0.92      6393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training and prediction results using TF-IDF vectoriser and five classifiers\n",
    "upsampled_train_tweets['lemmatized_tweet'].fillna('', inplace=True)\n",
    "test_data['lemmatized_tweet'].fillna('', inplace=True)\n",
    "\n",
    "#Naive Bayes\n",
    "TFIDF_NB_pipeline.fit(upsampled_train_tweets['lemmatized_tweet'], upsampled_train_tweets['label'])#Fit the NB pipeline to training data\n",
    "LEM_pred_NB = TFIDF_NB_pipeline.predict(test_data['lemmatized_tweet'])#Predict on the test data using NB\n",
    "LEM_accuracy_NB = accuracy_score(test_data['label'], LEM_pred_NB)\n",
    "LEM_train_pred_NB = TFIDF_NB_pipeline.predict(upsampled_train_tweets['lemmatized_tweet'])\n",
    "LEM_train_accuracy_NB = accuracy_score(upsampled_train_tweets['label'], LEM_train_pred_NB)\n",
    "\n",
    "#Logistic Regression\n",
    "TFIDF_LR_pipeline.fit(upsampled_train_tweets['lemmatized_tweet'], upsampled_train_tweets['label'])#Fit the LR pipeline to training data\n",
    "LEM_pred_LR = TFIDF_LR_pipeline.predict(test_data['lemmatized_tweet'])#Predict on the test data using LR\n",
    "LEM_accuracy_LR = accuracy_score(test_data['label'], LEM_pred_LR)\n",
    "LEM_train_pred_LR = TFIDF_LR_pipeline.predict(upsampled_train_tweets['lemmatized_tweet'])\n",
    "LEM_train_accuracy_LR = accuracy_score(upsampled_train_tweets['label'], LEM_train_pred_LR)\n",
    "\n",
    "#Support Vector Machine\n",
    "TFIDF_SVM_pipeline.fit(upsampled_train_tweets['lemmatized_tweet'], upsampled_train_tweets['label'])#Fit the SVM pipeline to training data\n",
    "LEM_pred_SVM = TFIDF_SVM_pipeline.predict(test_data['lemmatized_tweet'])#Predict on the test data using SVM\n",
    "LEM_accuracy_SVM = accuracy_score(test_data['label'], LEM_pred_SVM)\n",
    "LEM_train_pred_SVM = TFIDF_SVM_pipeline.predict(upsampled_train_tweets['lemmatized_tweet'])\n",
    "LEM_train_accuracy_SVM = accuracy_score(upsampled_train_tweets['label'], LEM_train_pred_SVM)\n",
    "\n",
    "#Random Forest\n",
    "TFIDF_RF_pipeline.fit(upsampled_train_tweets['lemmatized_tweet'], upsampled_train_tweets['label'])#Fit the RF pipeline to training data\n",
    "LEM_pred_RF = TFIDF_RF_pipeline.predict(test_data['lemmatized_tweet'])#Predict on the test data using RF\n",
    "LEM_accuracy_RF = accuracy_score(test_data['label'], LEM_pred_RF)\n",
    "LEM_train_pred_RF = TFIDF_RF_pipeline.predict(upsampled_train_tweets['lemmatized_tweet'])\n",
    "LEM_train_accuracy_RF = accuracy_score(upsampled_train_tweets['label'], LEM_train_pred_RF)\n",
    "\n",
    "#Decision Tree\n",
    "TFIDF_DT_pipeline.fit(upsampled_train_tweets['lemmatized_tweet'], upsampled_train_tweets['label'])#Fit the DT pipeline to training data\n",
    "LEM_pred_DT = TFIDF_DT_pipeline.predict(test_data['lemmatized_tweet'])#Predict on the test data using DT\n",
    "LEM_accuracy_DT = accuracy_score(test_data['label'], LEM_pred_DT)\n",
    "LEM_train_pred_DT = TFIDF_DT_pipeline.predict(upsampled_train_tweets['lemmatized_tweet'])\n",
    "LEM_train_accuracy_DT = accuracy_score(upsampled_train_tweets['label'], LEM_train_pred_DT)\n",
    "\n",
    "#Reports\n",
    "report_train_NB = classification_report(upsampled_train_tweets['label'], LEM_train_pred_NB, zero_division=0)\n",
    "report_NB = classification_report(test_data['label'], LEM_pred_NB, zero_division=0)\n",
    "# print(\"NB Classification Report (Train Dataset):\")\n",
    "# print(f\"NB Accuracy (Train Dataset): {LEM_train_accuracy_NB:.4f}\")\n",
    "# print(report_train_NB)\n",
    "print()\n",
    "print(\"NB Classification Report (Test Dataset):\")\n",
    "print(f\"NB Accuracy (Test Dataset): {LEM_accuracy_NB:.4f}\")\n",
    "print(report_NB)\n",
    "\n",
    "report_train_LR = classification_report(upsampled_train_tweets['label'], LEM_train_pred_LR, zero_division=0)\n",
    "report_LR = classification_report(test_data['label'], LEM_pred_LR, zero_division=0)\n",
    "# print(\"LR Classification Report (Train Dataset):\")\n",
    "# print(f\"LR Accuracy (Train Dataset): {LEM_train_accuracy_LR:.4f}\")\n",
    "# print(report_train_LR)\n",
    "print()\n",
    "print(\"LR Classification Report (Test Dataset):\")\n",
    "print(f\"LR Accuracy (Test Dataset): {LEM_accuracy_LR:.4f}\")\n",
    "print(report_LR)\n",
    "\n",
    "report_train_SVM = classification_report(upsampled_train_tweets['label'], LEM_train_pred_SVM, zero_division=0)\n",
    "report_SVM = classification_report(test_data['label'], LEM_pred_SVM, zero_division=0)\n",
    "# print(\"SVM Classification Report (Train Dataset):\")\n",
    "# print(f\"SVM Accuracy (Train Dataset): {LEM_train_accuracy_SVM:.4f}\")\n",
    "# print(report_train_SVM)\n",
    "print()\n",
    "print(\"SVM Classification Report (Test Dataset):\")\n",
    "print(f\"SVM Accuracy (Test Dataset): {LEM_accuracy_SVM:.4f}\")\n",
    "print(report_SVM)\n",
    "\n",
    "report_train_DT = classification_report(upsampled_train_tweets['label'], LEM_train_pred_DT, zero_division=0)\n",
    "report_DT = classification_report(test_data['label'], LEM_pred_DT, zero_division=0)\n",
    "# print(\"DT Classification Report (Train Dataset):\")\n",
    "# print(f\"DT Accuracy (Train Dataset): {LEM_train_accuracy_DT:.4f}\")\n",
    "# print(report_train_DT)\n",
    "print()\n",
    "print(\"DT Classification Report (Test Dataset):\")\n",
    "print(f\"DT Accuracy (Test Dataset): {LEM_accuracy_DT:.4f}\")\n",
    "print(report_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1139d2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Test Accuracy: 0.6175504458000939\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.60      0.75      5937\n",
      "           1       0.14      0.81      0.23       456\n",
      "\n",
      "    accuracy                           0.62      6393\n",
      "   macro avg       0.56      0.71      0.49      6393\n",
      "weighted avg       0.92      0.62      0.71      6393\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Model: Decision Tree\n",
      "Test Accuracy: 0.9103707179727827\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      5937\n",
      "           1       0.38      0.39      0.38       456\n",
      "\n",
      "    accuracy                           0.91      6393\n",
      "   macro avg       0.66      0.67      0.67      6393\n",
      "weighted avg       0.91      0.91      0.91      6393\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Model: Random Forest\n",
      "Test Accuracy: 0.9450961989676209\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      5937\n",
      "           1       0.78      0.32      0.45       456\n",
      "\n",
      "    accuracy                           0.95      6393\n",
      "   macro avg       0.87      0.66      0.71      6393\n",
      "weighted avg       0.94      0.95      0.93      6393\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Model: SVM\n",
      "Test Accuracy: 0.6292820272172689\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.62      0.76      5937\n",
      "           1       0.14      0.81      0.24       456\n",
      "\n",
      "    accuracy                           0.63      6393\n",
      "   macro avg       0.56      0.71      0.50      6393\n",
      "weighted avg       0.92      0.63      0.72      6393\n",
      "\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training, Prediction result using Word2Vec\n",
    "pipelines = [pipeline_lr, pipeline_dt, pipeline_rf, pipeline_svm]\n",
    "model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM']\n",
    "\n",
    "for pipeline, model_name in zip(pipelines, model_names):\n",
    "    # Train the model\n",
    "    pipeline.fit(upsampled_train_tweets['lemmatized_tweet'], upsampled_train_tweets['label'])  \n",
    "\n",
    "    # Predict on the test set\n",
    "    y_test_pred = pipeline.predict(test_data['lemmatized_tweet'])\n",
    "        \n",
    "    # Evaluate the model on test set\n",
    "    print(\"Model:\", model_name)\n",
    "    print(\"Test Accuracy:\", accuracy_score(test_data['label'], y_test_pred))\n",
    "    print(\"Test Classification Report:\\n\", classification_report(test_data['label'], y_test_pred, zero_division=0))\n",
    "    print(\"-----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7395b07f",
   "metadata": {},
   "source": [
    "### Approach 2. Vectorisation then Upsample (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "990446d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95      5937\n",
      "           1       0.44      0.87      0.59       456\n",
      "\n",
      "    accuracy                           0.91      6393\n",
      "   macro avg       0.72      0.89      0.77      6393\n",
      "weighted avg       0.95      0.91      0.93      6393\n",
      "\n",
      "Logistic Regression Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97      5937\n",
      "           1       0.58      0.81      0.67       456\n",
      "\n",
      "    accuracy                           0.94      6393\n",
      "   macro avg       0.78      0.88      0.82      6393\n",
      "weighted avg       0.96      0.94      0.95      6393\n",
      "\n",
      "Decision Tree Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      5937\n",
      "           1       0.41      0.54      0.46       456\n",
      "\n",
      "    accuracy                           0.91      6393\n",
      "   macro avg       0.69      0.74      0.71      6393\n",
      "weighted avg       0.92      0.91      0.92      6393\n",
      "\n",
      "SVM Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5937\n",
      "           1       0.90      0.52      0.66       456\n",
      "\n",
      "    accuracy                           0.96      6393\n",
      "   macro avg       0.93      0.76      0.82      6393\n",
      "weighted avg       0.96      0.96      0.96      6393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### upsampled, splitted, vectorised and algorithm merged.. using TFIDF\n",
    "\n",
    "# Read in the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatized_tweet(word):\n",
    "    word = re.sub(regex, '', word, flags=re.MULTILINE)  # Apply regex\n",
    "    word = word.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(word)  # Tokenize the text\n",
    "    tagged_tokens = pos_tag(tokens)  # Perform POS tagging\n",
    "    lemmatized_tokens = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        if token not in stop_words:\n",
    "            pos_lemma = get_wordnet_pos(pos)\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, pos=pos_lemma)\n",
    "            lemmatized_tokens.append(lemmatized_token)\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "df['lemmatized_tweet'] = df['tweet'].apply(lemmatized_tweet)\n",
    "# df.head(5)\n",
    "\n",
    "# Split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Upsample the train dataset (assuming class imbalance)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_majority = df_train[df_train['label'] == 0]\n",
    "df_minority = df_train[df_train['label'] == 1]\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "X_train_upsampled = df_upsampled['tweet']\n",
    "y_train_upsampled = df_upsampled['label']\n",
    "\n",
    "# Vectorize the upsampled training data\n",
    "X_train_upsampled_vectorized = vectorizer.transform(X_train_upsampled)\n",
    "\n",
    "# Implement Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_upsampled_vectorized, y_train_upsampled)\n",
    "nb_predictions = nb_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Implement Random Forest classifier\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train_upsampled_vectorized, y_train_upsampled)\n",
    "lr_predictions = lr_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Implement Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_classifier.fit(X_train_upsampled_vectorized, y_train_upsampled)\n",
    "dt_predictions = dt_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Implement SVM classifier\n",
    "svm_classifier2 = SVC()\n",
    "svm_classifier2.fit(X_train_upsampled_vectorized, y_train_upsampled)\n",
    "svm_predictions = svm_classifier2.predict(X_test_vectorized)\n",
    "\n",
    "# Print classification report for Naive Bayes\n",
    "print(\"Naive Bayes Classifier:\")\n",
    "print(classification_report(y_test, nb_predictions))\n",
    "\n",
    "# Print classification report for Random Forest\n",
    "print(\"Logistic Regression Classifier:\")\n",
    "print(classification_report(y_test, lr_predictions))\n",
    "\n",
    "# Print classification report for Decision Tree\n",
    "print(\"Decision Tree Classifier:\")\n",
    "print(classification_report(y_test, dt_predictions))\n",
    "\n",
    "# Print classification report for SVM\n",
    "print(\"SVM Classifier:\")\n",
    "print(classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9065e",
   "metadata": {},
   "source": [
    "###  Vectorisation then Upsample (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57ccc5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.60      0.74      5937\n",
      "           1       0.13      0.79      0.22       456\n",
      "\n",
      "    accuracy                           0.61      6393\n",
      "   macro avg       0.55      0.69      0.48      6393\n",
      "weighted avg       0.91      0.61      0.70      6393\n",
      "\n",
      "Decision Tree Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      5937\n",
      "           1       0.37      0.39      0.38       456\n",
      "\n",
      "    accuracy                           0.91      6393\n",
      "   macro avg       0.66      0.67      0.66      6393\n",
      "weighted avg       0.91      0.91      0.91      6393\n",
      "\n",
      "Random Forest Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      5937\n",
      "           1       0.80      0.32      0.45       456\n",
      "\n",
      "    accuracy                           0.95      6393\n",
      "   macro avg       0.88      0.65      0.71      6393\n",
      "weighted avg       0.94      0.95      0.93      6393\n",
      "\n",
      "SVM Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.61      0.75      5937\n",
      "           1       0.14      0.79      0.23       456\n",
      "\n",
      "    accuracy                           0.63      6393\n",
      "   macro avg       0.56      0.70      0.49      6393\n",
      "weighted avg       0.91      0.63      0.72      6393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(regex, '', tweet, flags=re.MULTILINE)  # Apply regex\n",
    "    tweet = tweet.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(tweet)  # Tokenize the text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "# Apply preprocessing to tweets\n",
    "df['processed_tweet'] = df['tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# Split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['processed_tweet'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to vectorize a tweet using Word2Vec\n",
    "def vectorize_tweet(tweet):\n",
    "    vectors = []\n",
    "    for token in tweet:\n",
    "        if token in word2vec_model.wv:\n",
    "            vectors.append(word2vec_model.wv[token])\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# Vectorize the train and test datasets\n",
    "X_train_vectorized = [vectorize_tweet(tweet) for tweet in X_train]\n",
    "X_test_vectorized = [vectorize_tweet(tweet) for tweet in X_test]\n",
    "\n",
    "# Convert vectorized data to numpy arrays\n",
    "X_train_vectorized = np.array(X_train_vectorized)\n",
    "X_test_vectorized = np.array(X_test_vectorized)\n",
    "\n",
    "# Upsample the train dataset (assuming class imbalance)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_majority = df_train[df_train['label'] == 0]\n",
    "df_minority = df_train[df_train['label'] == 1]\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "X_train_upsampled = df_upsampled['processed_tweet']\n",
    "y_train_upsampled = df_upsampled['label']\n",
    "\n",
    "# Vectorize the upsampled training data\n",
    "X_train_upsampled_vectorized = [vectorize_tweet(tweet) for tweet in X_train_upsampled]\n",
    "X_train_upsampled_vectorized = np.array(X_train_upsampled_vectorized)\n",
    "\n",
    "# Implement SVM classifier\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train_upsampled_vectorized, y_train_upsampled)\n",
    "svm_predictions = svm_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Implement Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train_upsampled_vectorized, y_train_upsampled)\n",
    "lr_predictions = lr_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Implement Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_classifier.fit(X_train_upsampled_vectorized, y_train_upsampled)\n",
    "dt_predictions = dt_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Implement Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train_upsampled_vectorized, y_train_upsampled)\n",
    "rf_predictions = rf_classifier.predict(X_test_vectorized)\n",
    "\n",
    "#print classificatiion reports\n",
    "print(\"Logistic Regression Classifier:\")\n",
    "print(classification_report(y_test, lr_predictions))\n",
    "print(\"Decision Tree Classifier:\")\n",
    "print(classification_report(y_test, dt_predictions))\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "print(\"SVM Classifier:\")\n",
    "print(classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6bc913",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "In this section, we will train a deep learning model using the cleaned and normalized tweet dataset. The code below outlines the steps involved using tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "649783b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "800/800 [==============================] - 267s 323ms/step - loss: 0.1994 - accuracy: 0.9399 - val_loss: 0.1639 - val_accuracy: 0.9478\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 258s 323ms/step - loss: 0.1023 - accuracy: 0.9666 - val_loss: 0.1662 - val_accuracy: 0.9485\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 250s 313ms/step - loss: 0.0616 - accuracy: 0.9805 - val_loss: 0.2024 - val_accuracy: 0.9474\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 165s 206ms/step - loss: 0.0438 - accuracy: 0.9864 - val_loss: 0.2143 - val_accuracy: 0.9429\n",
      "800/800 [==============================] - 32s 39ms/step\n",
      "200/200 [==============================] - 8s 38ms/step\n",
      "Test Accuracy: 0.9477553574221805\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      5937\n",
      "           1       0.74      0.41      0.53       456\n",
      "\n",
      "    accuracy                           0.95      6393\n",
      "   macro avg       0.85      0.70      0.75      6393\n",
      "weighted avg       0.94      0.95      0.94      6393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_sequence_length = 100  # Define the maximum length for padding\n",
    "X_train_padded = pad_sequences(X_train_tokens, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_tokens, maxlen=max_sequence_length)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "    tf.keras.layers.LSTM(units=128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.3),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train_padded, y_train, batch_size=32, epochs=10, validation_data=(X_test_padded, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred_probs = model.predict(X_train_padded)\n",
    "y_test_pred_probs = model.predict(X_test_padded)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_train_pred = [1 if pred > 0.5 else 0 for pred in y_train_pred_probs]\n",
    "y_test_pred = [1 if pred > 0.5 else 0 for pred in y_test_pred_probs]\n",
    "\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Classification Report:\\n\", test_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d892a",
   "metadata": {},
   "source": [
    "In the below code, we upsample our dataset before using training our deep learning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b17fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1487/1487 [==============================] - 380s 252ms/step - loss: 0.1957 - accuracy: 0.9206 - val_loss: 1.0539 - val_accuracy: 0.7419\n",
      "Epoch 2/10\n",
      "1487/1487 [==============================] - 464s 312ms/step - loss: 0.0517 - accuracy: 0.9821 - val_loss: 1.4350 - val_accuracy: 0.7737\n",
      "Epoch 3/10\n",
      "1487/1487 [==============================] - 509s 342ms/step - loss: 0.0328 - accuracy: 0.9878 - val_loss: 1.5269 - val_accuracy: 0.7974\n",
      "Epoch 4/10\n",
      "1487/1487 [==============================] - 487s 327ms/step - loss: 0.0268 - accuracy: 0.9905 - val_loss: 1.7189 - val_accuracy: 0.7708\n",
      "200/200 [==============================] - 14s 64ms/step\n",
      "Test Accuracy: 0.7419052088221493\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.79      0.85      5937\n",
      "           1       0.06      0.17      0.09       456\n",
      "\n",
      "    accuracy                           0.74      6393\n",
      "   macro avg       0.49      0.48      0.47      6393\n",
      "weighted avg       0.86      0.74      0.80      6393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Upsample the train dataset (assuming class imbalance)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_majority = df_train[df_train['label'] == 0]\n",
    "df_minority = df_train[df_train['label'] == 1]\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "X_train_upsampled = df_upsampled['processed_tweet']\n",
    "y_train_upsampled = df_upsampled['label']\n",
    "\n",
    "# Tokenize the upsampled train data\n",
    "tokenizer.fit_on_texts(X_train_upsampled)\n",
    "X_train_upsampled_tokens = tokenizer.texts_to_sequences(X_train_upsampled)\n",
    "X_train_upsampled_padded = pad_sequences(X_train_upsampled_tokens, maxlen=max_sequence_length)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "    tf.keras.layers.LSTM(units=128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.3),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train_upsampled_padded, y_train_upsampled, batch_size=32, epochs=10, validation_data=(X_test_padded, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Predict on the training and testing sets\n",
    "# y_train_pred_probs = model.predict(X_train_padded)\n",
    "y_test_pred_probs = model.predict(X_test_padded)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "# y_train_pred = [1 if pred > 0.5 else 0 for pred in y_train_pred_probs]\n",
    "y_test_pred = [1 if pred > 0.5 else 0 for pred in y_test_pred_probs]\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "# train_report = classification_report(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# print(\"Train Accuracy:\", train_accuracy)\n",
    "# print(\"Train Classification Report:\\n\", train_report)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Classification Report:\\n\", test_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de2195",
   "metadata": {},
   "source": [
    "# Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbd8925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:1000/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:1000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:07] \"GET /_alive_4df90692-28d6-4130-ba02-d869df760182 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:1000/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x24df2a07ca0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [11/Oct/2024 15:02:07] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:07] \"GET /_dash-component-suites/dash/deps/polyfill@7.v2_9_2m1680261794.12.1.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:07] \"GET /_dash-component-suites/dash/deps/react@16.v2_9_2m1680261794.14.0.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:07] \"GET /_dash-component-suites/dash/deps/react-dom@16.v2_9_2m1680261794.14.0.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:07] \"GET /_dash-component-suites/dash/deps/prop-types@15.v2_9_2m1680261794.8.1.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:07] \"GET /_dash-component-suites/dash/dash-renderer/build/dash_renderer.v2_9_2m1680261794.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:07] \"GET /_dash-component-suites/dash/dcc/dash_core_components.v2_9_1m1680261794.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:07] \"GET /_dash-component-suites/dash/html/dash_html_components.v2_0_10m1680261795.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:07] \"GET /_dash-component-suites/dash/dcc/dash_core_components-shared.v2_9_1m1680261794.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:07] \"GET /_dash-component-suites/dash/dash_table/bundle.v5_2_4m1680261794.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:08] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:08] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:02:08] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:47:04] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:47:05] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:47:16] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:47:17] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:47:17] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:47:26] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2024 15:47:37] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Create the app\n",
    "app = jupyter_dash.JupyterDash(__name__)\n",
    "image_path = 'logo.png'\n",
    "with open(image_path, 'rb') as f:\n",
    "    encoded_image = base64.b64encode(f.read()).decode()\n",
    "\n",
    "# Header component\n",
    "header = html.Header(\n",
    "    children=[\n",
    "        html.H1(\"Hate Speech Detection\", style={'float': 'right', 'paddingLeft': '180px', 'paddingBottom': '20px', 'marginBottom': '0', 'width': '90%'}),\n",
    "        html.Div(\n",
    "            children=[\n",
    "                html.Img(src='data:logo/png;base64,{}'.format(encoded_image), style={'width': '100%', 'height': '100%'})\n",
    "            ],\n",
    "            style={'width': '6%', 'float': 'right', 'height': '50px', 'paddingRight': '10px'}\n",
    "        )\n",
    "    ],\n",
    "    style={\"color\": \"white\", \"paddingTop\": \"20px\", \"backgroundColor\": \"#000000\", \"textAlign\": \"center\", \"display\": \"flex\", \"justifyContent\": \"center\", \"width\": \"100%\"}\n",
    ")\n",
    "# Footer component\n",
    "footer = html.Footer(\n",
    "    html.P(\"© 2023 A Hate Speech Detection Prototype.\"),\n",
    "    style={\n",
    "                \"bottom\": \"0\",\n",
    "                \"width\": \"100%\",\n",
    "                \"height\": \"60px\",\n",
    "                \"backgroundColor\": \"#000000\",\n",
    "                \"color\": \"#fff\",\n",
    "                \"textAlign\": \"center\",\n",
    "                \"paddingTop\": \"20px\",\n",
    "            }\n",
    ")\n",
    "app.layout = html.Div([\n",
    "    header,\n",
    "    html.Div([\n",
    "        dcc.Input(id='text', type=\"text\", placeholder=\"Enter your text\", value=\" \", style={'width': '600px'}),\n",
    "        html.Button('Detect', id='detect-button', style={'marginTop': '10px'}),\n",
    "        html.Div(id='prediction-output')\n",
    "    ],\n",
    "    style={\"backgroundColor\": \"#f2f2f2\", \"textAlign\": \"center\", 'height': '60vh', 'display': 'flex', 'width': '100%', 'flexDirection': 'column', 'alignItems': 'center', 'justifyContent': 'center'}),\n",
    "    html.Div([\n",
    "        html.P(\"\"\"Disclaimer:\n",
    "            The hate speech detection prototype was engineered for demonstrative purposes only and is not intended for real-world application without further refinement. It is worth noting that this prototype may not possess optimal generalisation capabilities due to limitations in its dataset and the sampling technique employed during its development.\n",
    "            \n",
    "            The model's performance in detecting hate speech may be influenced by the dataset it was trained on, which might not encompass the full spectrum of hate speech variations present in the real world.\"\"\")\n",
    "    ]),\n",
    "    footer\n",
    "])\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 3)) \n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "tweets = pd.read_csv('train_set.csv')\n",
    "\n",
    "# Preprocess dataset to handle NaN values\n",
    "tweets['lemmatized_tweet'].fillna('', inplace=True)\n",
    "\n",
    "# Extract the tweet text and labels from the DataFrame\n",
    "training_data = tweets['lemmatized_tweet'].tolist()\n",
    "training_labels = tweets['label'].tolist()\n",
    "\n",
    "# Fit the vectorizer on training data\n",
    "vectorizer.fit(training_data)\n",
    "\n",
    "# Train the Support Vector Machine model\n",
    "svm_model = SVC()\n",
    "text_vectorized_train = vectorizer.transform(training_data)\n",
    "svm_model.fit(text_vectorized_train, training_labels)\n",
    "\n",
    "# joblib.dump(clf, 'svm_model.joblib')\n",
    "\n",
    "# # Load the model from joblib\n",
    "# clf_model = joblib.load('svm_model.joblib')\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('prediction-output', 'children'),\n",
    "    [Input('detect-button', 'n_clicks')],\n",
    "    [State('text', 'value')]\n",
    ")\n",
    "def detect_hate_speech(n_clicks, text):\n",
    "    if text:\n",
    "        processed = lemmatized_tweet(text)\n",
    "        text_vectorized = vectorizer.transform([processed])\n",
    "        text_vectorized_dense = text_vectorized.toarray()\n",
    "        prediction = svm_model.predict(text_vectorized)[0]\n",
    "        \n",
    "        if prediction == 1:\n",
    "            result = html.H3(\"Hate Speech Detected!\", style={'color': 'red'})\n",
    "        else:\n",
    "            result = html.H3(\"Non-Hate Speech\", style={'color': 'green'})\n",
    "    else:\n",
    "        result = html.H3(\"Please enter some text\")\n",
    "    \n",
    "    return result  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(mode='inline', debug=False, port=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a643b",
   "metadata": {},
   "source": [
    "#                                                          The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
